{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data upload, Preprocessing, KGPT2 weight load, Import package"
      ],
      "metadata": {
        "id": "EdDMhBHVx3W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "!pip install torchtext\n",
        "!pip install tensorboardcolab"
      ],
      "metadata": {
        "id": "k3O__DBtXdBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzC78N7pWjHe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "import re\n",
        "#from pytorch_lightning.core.lightning import LightningModule\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "from transformers import PreTrainedTokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoTkKZSmW6WQ",
        "outputId": "25e8cb7b-0a77-4f08-e671-a7594ea6465c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 30 06:35:17 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') \n",
        "tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_edrWV-eW74j",
        "outputId": "8155f252-4506-4cb7-bea0-303d2102448f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁안녕',\n",
              " '하',\n",
              " '세',\n",
              " '요.',\n",
              " '▁한국어',\n",
              " '▁G',\n",
              " 'P',\n",
              " 'T',\n",
              " '-2',\n",
              " '▁입',\n",
              " '니다.',\n",
              " '😤',\n",
              " ':)',\n",
              " 'l^o']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
      ],
      "metadata": {
        "id": "R12sWbUFXrBe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '근육이 커지기 위해서는'\n",
        "input_ids = tokenizer.encode(text)\n",
        "gen_ids = model.generate(torch.tensor([input_ids]),\n",
        "                           max_length=128,\n",
        "                           repetition_penalty=2.0,\n",
        "                           pad_token_id=tokenizer.pad_token_id,\n",
        "                           eos_token_id=tokenizer.eos_token_id,\n",
        "                           bos_token_id=tokenizer.bos_token_id,\n",
        "                           use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu9aTcdFXEu3",
        "outputId": "751815f5-10fc-4ec2-bb69-e5ac02c105e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\n",
            "특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\n",
            "또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\n",
            "아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\n",
            "운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\n",
            "운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\n",
            "운동을\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_TKN = \"<usr>\"\n",
        "A_TKN = \"<sys>\"\n",
        "BOS = '</s>'\n",
        "EOS = '</s>'\n",
        "MASK = '<unused0>'\n",
        "SENT = '<unused1>'\n",
        "PAD = '<pad>'"
      ],
      "metadata": {
        "id": "JOASRGT5XTf3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "            bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
        "            pad_token=PAD, mask_token=MASK) \n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc57-NUyZy4U",
        "outputId": "4391dd5f-de1e-46d1-8802-3cad7b2ab861"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "cBbT4qRxyLwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\",\n",
        "    filename=\"ChatBotData.csv\",\n",
        ")\n",
        "Chatbot_Data = pd.read_csv(\"ChatBotData.csv\")\n",
        "# Test 용으로 300개 데이터만 처리한다.\n",
        "Chatbot_Data = Chatbot_Data[:300]\n",
        "Chatbot_Data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nMhcjgw8Z1xW",
        "outputId": "c792eb9e-6f44-4e1f-acdc-76b47513fdfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc23ec16-0c07-4bbb-b853-a4c17a1ff255\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc23ec16-0c07-4bbb-b853-a4c17a1ff255')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc23ec16-0c07-4bbb-b853-a4c17a1ff255 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc23ec16-0c07-4bbb-b853-a4c17a1ff255');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention transformer model"
      ],
      "metadata": {
        "id": "IsVwRddyyR3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 챗봇 데이터를 처리하는 클래스를 만든다.\n",
        "class ChatbotDataset(Dataset):\n",
        "    def __init__(self, chats, max_len=40):  # 데이터셋의 전처리를 해주는 부분\n",
        "        self._data = chats\n",
        "        self.max_len = max_len\n",
        "        self.q_token = Q_TKN\n",
        "        self.a_token = A_TKN\n",
        "        self.sent_token = SENT\n",
        "        self.eos = EOS\n",
        "        self.mask = MASK\n",
        "        self.tokenizer = koGPT2_TOKENIZER\n",
        "\n",
        "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
        "        return len(self._data)\n",
        "\n",
        "    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n",
        "        turn = self._data.iloc[idx]\n",
        "        q = turn[\"Q\"]  # 질문을 가져온다.\n",
        "        q = re.sub(r\"([?.!,])\", r\" \", q)  # 구둣점들을 제거한다.\n",
        "\n",
        "        a = turn[\"A\"]  # 답변을 가져온다.\n",
        "        a = re.sub(r\"([?.!,])\", r\" \", a)  # 구둣점들을 제거한다.\n",
        "\n",
        "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n",
        "        q_len = len(q_toked)\n",
        "\n",
        "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
        "        a_len = len(a_toked)\n",
        "\n",
        "        #질문의 길이가 최대길이보다 크면\n",
        "        if q_len > self.max_len:\n",
        "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
        "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
        "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
        "                q_len = len(q_toked)\n",
        "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
        "            a_toked = a_toked[:a_len]\n",
        "            a_len = len(a_toked)\n",
        "\n",
        "        #질문의 길이 + 답변의 길이가 최대길이보다 크면\n",
        "        if q_len + a_len > self.max_len:\n",
        "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
        "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
        "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
        "                q_len = len(q_toked)\n",
        "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
        "            a_toked = a_toked[:a_len]\n",
        "            a_len = len(a_toked)\n",
        "\n",
        "        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n",
        "        labels = [self.mask,] * q_len + a_toked[1:]\n",
        "\n",
        "        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n",
        "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
        "        # 답변 labels을 index 로 만든다.\n",
        "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
        "        # 최대길이만큼 PADDING\n",
        "        while len(labels_ids) < self.max_len:\n",
        "            labels_ids += [self.tokenizer.pad_token_id]\n",
        "\n",
        "        # 질문 + 답변을 index 로 만든다.    \n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
        "        # 최대길이만큼 PADDING\n",
        "        while len(token_ids) < self.max_len:\n",
        "            token_ids += [self.tokenizer.pad_token_id]\n",
        "\n",
        "        #질문+답변, 마스크, 답변\n",
        "        return (token_ids, np.array(mask), labels_ids)"
      ],
      "metadata": {
        "id": "IsT96OzvbqUv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    mask = [item[1] for item in batch]\n",
        "    label = [item[2] for item in batch]\n",
        "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)"
      ],
      "metadata": {
        "id": "Mui3w6tnbrNA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = ChatbotDataset(Chatbot_Data, max_len=40)\n",
        "\n",
        "#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\n",
        "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)"
      ],
      "metadata": {
        "id": "N3ISaKfzbub8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"start\")\n",
        "for batch_idx, samples in enumerate(train_dataloader):\n",
        "    token_ids, mask, label = samples\n",
        "    print(\"token_ids ====> \", token_ids)\n",
        "    print(\"mask =====> \", mask)\n",
        "    print(\"label =====> \", label)\n",
        "print(\"end\")"
      ],
      "metadata": {
        "id": "AVAInnWybuex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_set = ChatbotDataset(Chatbot_Data, max_len=40)\n",
        "#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\n",
        "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)"
      ],
      "metadata": {
        "id": "-9geOAMkZ5P8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrsG8p6oaIB0",
        "outputId": "93d9b759-790c-402c-b554-c8cd8d4e5880"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(51200, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-5\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 100\n",
        "Sneg = -1e18\n"
      ],
      "metadata": {
        "id": "YtNY0bGpb_Yf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reloaded to GPU"
      ],
      "metadata": {
        "id": "LsxqD2e3yhcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"start\")\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, samples in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids, mask, label = samples\n",
        "\n",
        "        # Move tensors to the same device as the model\n",
        "        token_ids = token_ids.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        out = model(token_ids)\n",
        "        out = out.logits\n",
        "\n",
        "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
        "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
        "        loss = criterion(mask_out.transpose(2, 1), label)\n",
        "\n",
        "        avg_loss = loss.sum() / mask.sum()\n",
        "        avg_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "print(\"end\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSSMFi_NcClt",
        "outputId": "10f005d2-5fd8-4be2-f5a3-dc9355cc2fe3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2QVFyAxB5QZsR6riD2FgTJT09CD_5BxGcgEFEXJidcJdXPEVq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnyAuYMBsL9K",
        "outputId": "c75473cc-8960-4382-c10d-16074560e243"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training step"
      ],
      "metadata": {
        "id": "Xf-A26Jsz5AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "# tbc = TensorBoardColab()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "AVG_loss = []\n",
        "for_loss = []\n",
        "print(\"start\")\n",
        "\n",
        "# Create a SummaryWriter for TensorBoard logging\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, samples in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids, mask, label = samples\n",
        "\n",
        "        # Move tensors to the same device as the model\n",
        "        token_ids = token_ids.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        out = model(token_ids)\n",
        "        out = out.logits\n",
        "\n",
        "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
        "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
        "        loss = criterion(mask_out.transpose(2, 1), label)\n",
        "        for_loss.append(loss)\n",
        "\n",
        "        avg_loss = loss.sum() / mask.sum()\n",
        "        avg_loss.backward()\n",
        "        optimizer.step()\n",
        "        AVG_loss.append(avg_loss)\n",
        "\n",
        "        # Log loss to TensorBoard\n",
        "        writer.add_scalar(\"Loss\", avg_loss.item(), epoch * len(train_dataloader) + batch_idx)\n",
        "\n",
        "        # Log model weights to TensorBoard\n",
        "        # for name, param in model.named_parameters():\n",
        "        #     writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch * len(train_dataloader) + batch_idx)\n",
        "\n",
        "        # Flush the writer to make sure all pending events are written to disk\n",
        "        # writer.flush()\n",
        "\n",
        "print(\"end\")\n",
        "\n",
        "# Close the SummaryWriter\n",
        "# writer.close()\n",
        "\n",
        "# # Close the TensorBoardColab instance\n",
        "# tbc.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErtdbxmBgUfj",
        "outputId": "bd48a7f0-7114-4349-bce3-b4c9d45ade80"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import torch\n",
        "\n",
        "# # Copy tensors to CPU and detach from computation graph\n",
        "# for_loss_cpu = [item.detach().cpu().numpy() for item in for_loss]\n",
        "# AVG_loss_cpu = [item.detach().cpu().numpy() for item in AVG_loss]\n",
        "\n",
        "# # loss history\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.title('Loss Progress')\n",
        "# #plt.plot(for_loss_cpu, label='Loss')\n",
        "# plt.plot(AVG_loss_cpu, label='Avg Loss')\n",
        "# plt.xlabel('batch count')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "5yFEt2COvJIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference step"
      ],
      "metadata": {
        "id": "crzPwu9AyoUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    while True:\n",
        "        q = input(\"user > \").strip()\n",
        "        if q == \"quit\":\n",
        "            break\n",
        "        a = \"\"\n",
        "        while True:\n",
        "            input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0)\n",
        "\n",
        "            # Move the input tensor to the same device as the model\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            pred = model(input_ids)\n",
        "            pred = pred.logits\n",
        "\n",
        "            # Move the tensor from CUDA device to CPU and convert to NumPy array\n",
        "            gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().cpu().numpy().tolist())[-1]\n",
        "\n",
        "            if gen == EOS:\n",
        "                break\n",
        "            a += gen.replace(\"▁\", \" \")\n",
        "        print(\"Chatbot > {}\".format(a.strip()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjgnAh_tcElj",
        "outputId": "d1ea2664-3b7c-495f-e660-cb1a75990880"
      },
      "execution_count": 44,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user > 오늘 너무 힘들다\n",
            "Chatbot > 벗어나는 게 좋겠네요\n",
            "user > 벗어나고싶어\n",
            "Chatbot > 빨리 집에 돌아가서 끄고 나오세요\n",
            "user > 내가 썸타는 여자가 있는데 어떻게 해야될까?\n",
            "Chatbot > 남자사람친구  여자사람친구 하세요\n",
            "user > 왜 ㅠ 내가 고백 박아버리면 안되?\n",
            "Chatbot > 후회는 후회를 낳을뿐이에요  용기 내세요\n",
            "user > 와.. 넌 진짜 못됬다\n",
            "Chatbot > 되도록 만나지 마세요\n",
            "user > 흥! 우리 헤어져\n",
            "Chatbot > 벗어나는 게 좋겠네요\n",
            "user > 혹시 엄준식의 전설에 대해서 아니?\n",
            "Chatbot > 그건 습관이에요\n",
            "user > 그래..그건 습관이지..\n",
            "Chatbot > 저도 좋아해주세요\n",
            "user > 벌써 12시야\n",
            "Chatbot > 하루가 또 가네요\n",
            "user > 지금 뭐해야 좋을까?\n",
            "Chatbot > 지금처럼 잘될 거예요\n",
            "user > 지금 심심한데 뭘 하는게 좋을까?\n",
            "Chatbot > 먼저 생활패턴을 살펴 보세요\n",
            "user > 생활패턴을 살펴봤는데 난 오늘 저녁에 게임을 해야겠어 너 생각은 어때?\n",
            "Chatbot > 게임할때는 시간이 더 빨리 가요\n",
            "user > 그렇긴 하지.. 넌 게임 좋아해?\n",
            "Chatbot > 다른 게임해보세요\n",
            "user > 무슨 게임을 해봐야 될까?\n",
            "Chatbot > 다른 게임해보세요\n",
            "user > 무슨 다른 게임을 해봐야 할까?\n",
            "Chatbot > 다른 게임해보세요\n",
            "user > 왜 계속 똑같은 말만 하는거야\n",
            "Chatbot > 같은 조가 되길 바랄게요\n",
            "user > 너 진짜 뒤지고 싶어?\n",
            "Chatbot > 그럴 필요 없어요\n",
            "user > 아 그렇지.. 넌 살아있는게 아니니까 뒤질 필요가 없긴 하지... 너 제법 똑똑한데?\n",
            "Chatbot > 먼저 생활패턴을 살펴 보세요\n",
            "user > 나 지금 좀 배고파\n",
            "Chatbot > 이럴 때 잘 쉬는 게 중요해요\n",
            "user > 오늘 좀 쉬어야 겠다..\n",
            "Chatbot > 벗어나는 게 좋겠네요\n",
            "user > quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qY5QhdeKyvbz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}